{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I benchmark a few text categorization models to test whehter word embeddings like word2vec can improve text classification accuracy. The the notebook proceeds as follows:\n",
    "1. downloading the datasets\n",
    "2. construction of the training set\n",
    "3. definitions of models\n",
    "4. benchmarking models\n",
    "5. plotting results\n",
    "\n",
    "This notebook was converted to Python 3 by <a href='https://github.com/marcelobeckmann'>@marcelobeckmann</a>\n",
    "\n",
    "Tested with Python 3.5.3. The results are the same from the notebook for Python 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading datasets and pretrained wector embeddings. Especially the embeddings can take a while to download. You might want run these in the terminal instead to see wget's progress bar. If you're on Windows (and not in cygwin) %%bash cell magic won't work and you'll have to do all this manually (or with %%cmd magic I guess). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 2: wget: command not found\n",
      "bash: line 3: wget: command not found\n",
      "cat: r8-*-no-stop.txt: No such file or directory\n",
      "bash: line 6: wget: command not found\n",
      "bash: line 7: wget: command not found\n",
      "cat: r52-*-no-stop.txt: No such file or directory\n",
      "bash: line 9: wget: command not found\n",
      "bash: line 10: wget: command not found\n",
      "cat: 20ng-*-no-stop.txt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# download Reuters' text categorization benchmarks\n",
    "# wget http://www.cs.umb.edu/~smimarog/textmining/datasets/r8-train-no-stop.txt\n",
    "# wget http://www.cs.umb.edu/~smimarog/textmining/datasets/r8-test-no-stop.txt\n",
    "# concatenate train and test files, we'll make our own train-test splits\n",
    "# cat r8-*-no-stop.txt > r8-no-stop.txt\n",
    "# wget http://www.cs.umb.edu/~smimarog/textmining/datasets/r52-train-no-stop.txt\n",
    "# wget http://www.cs.umb.edu/~smimarog/textmining/datasets/r52-test-no-stop.txt\n",
    "# cat r52-*-no-stop.txt > r52-no-stop.txt\n",
    "# wget http://www.cs.umb.edu/~smimarog/textmining/datasets/20ng-test-no-stop.txt\n",
    "# wget http://www.cs.umb.edu/~smimarog/textmining/datasets/20ng-train-no-stop.txt\n",
    "# cat 20ng-*-no-stop.txt > 20ng-no-stop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "# download GloVe word vector representations\n",
    "# bunch of small embeddings - trained on 6B tokens - 822 MB download, 2GB unzipped\n",
    "# wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# zip = zipfile.ZipFile('glove.6B.zip')\n",
    "# zip.extractall()\n",
    "\n",
    "# and a single behemoth - trained on 840B tokens - 2GB compressed, 5GB unzipped\n",
    "# wget http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "# zip = zipfile.ZipFile('glove.840B.300d.zip')\n",
    "# zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "# TRAIN_SET_PATH = \"20ng-no-stop.txt\"\n",
    "# TRAIN_SET_PATH = \"r52-all-terms.txt\"\n",
    "TRAIN_SET_PATH = \"/Users/umair/text-mining/events_corpus.csv\"\n",
    "\n",
    "GLOVE_6B_50D_PATH = \"glove.6B.50d.txt\"\n",
    "GLOVE_840B_300D_PATH = \"glove.840B.300d.txt\"\n",
    "encoding=\"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 644761\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "with open(TRAIN_SET_PATH, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        label, text = line.split(\",\")\n",
    "        # texts are already tokenized, just split on space\n",
    "        # in a real case we would use e.g. spaCy for tokenization\n",
    "        # and maybe remove stopwords etc.\n",
    "        X.append(text.split())\n",
    "        y.append(label)\n",
    "X, y = np.array(X), np.array(y)\n",
    "print (\"total examples %s\" % len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare word embeddings - both the downloaded pretrained ones and train a new one from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open(GLOVE_6B_50D_PATH, \"rb\") as lines:\n",
    "    wvec = {line.split()[0].decode(encoding): np.array(line.split()[1:],dtype=np.float32)\n",
    "               for line in lines}\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading glove files, this may take a while\n",
    "# we're reading line by line and only saving vectors\n",
    "# that correspond to words from our training set\n",
    "# if you wan't to play around with the vectors and have \n",
    "# enough RAM - remove the 'if' line and load everything\n",
    "\n",
    "import struct \n",
    "\n",
    "glove_small = {}\n",
    "all_words = set(w for words in X for w in words)\n",
    "with open(GLOVE_6B_50D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_small[word] = nums\n",
    "\n",
    "            \n",
    "glove_big = {}\n",
    "with open(GLOVE_840B_300D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if word in all_words:\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_big[word] = nums\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'zip'>\n"
     ]
    }
   ],
   "source": [
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "print(zip)\n",
    "model = Word2Vec(X, size=100, window=5, min_count=5, workers=2)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.vectors)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715930\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with the classics - naive bayes of the multinomial and bernoulli varieties\n",
    "# with either pure counts or tfidf features\n",
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "# SVM - which is supposed to be more or less state of the art \n",
    "# http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\n",
    "svc = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
    "svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now the meat - classifiers using vector embeddings. We will implement an embedding vectorizer - a counterpart of CountVectorizer and TfidfVectorizer - that is given a word -> vector mapping and vectorizes texts by taking the mean of all the vectors corresponding to individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "    \n",
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "etree_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_big_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "benchmark all the things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "all_models = [\n",
    "    (\"mult_nb\", mult_nb),\n",
    "    (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "    (\"bern_nb\", bern_nb),\n",
    "    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"svc\", svc),\n",
    "    (\"svc_tfidf\", svc_tfidf),\n",
    "    (\"w2v\", etree_w2v),\n",
    "    (\"w2v_tfidf\", etree_w2v_tfidf),\n",
    "    (\"glove_small\", etree_glove_small),\n",
    "    (\"glove_small_tfidf\", etree_glove_small_tfidf),\n",
    "    (\"glove_big\", etree_glove_big),\n",
    "    (\"glove_big_tfidf\", etree_glove_big_tfidf),\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, X, y, cv=5).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "sns.barplot(x=[name for name, _ in scores], y=[score for _, score in scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, this is how it is. Let's see how the ranking depends on the amount of training data. Word embedding models which are semi-supervised should shine when there is very little labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model, X, y, n):\n",
    "    test_size = 1 - (n / float(len(y)))\n",
    "    scores = []\n",
    "    for train, test in StratifiedShuffleSplit(y, n_iter=5, test_size=test_size):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        scores.append(accuracy_score(model.fit(X_train, y_train).predict(X_test), y_test))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = [10, 40, 160, 640, 3200, 6400]\n",
    "table = []\n",
    "for name, model in all_models:\n",
    "    for n in train_sizes:\n",
    "        table.append({'model': name, \n",
    "                      'accuracy': benchmark(model, X, y, n), \n",
    "                      'train_size': n})\n",
    "df = pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "fig = sns.pointplot(x='train_size', y='accuracy', hue='model', \n",
    "                    data=df[df.model.map(lambda x: x in [\"mult_nb\", \"svc_tfidf\", \"w2v_tfidf\", \n",
    "                                                         \"glove_small_tfidf\", \"glove_big_tfidf\", \n",
    "                                                        ])])\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "fig.set(ylabel=\"accuracy\")\n",
    "fig.set(xlabel=\"labeled training examples\")\n",
    "fig.set(title=\"R8 benchmark\")\n",
    "fig.set(ylabel=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's be lazy and instead of refactoring, just copy the above code changing the input path to reuters 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_PATH = \"r52-no-stop.txt\"\n",
    "\n",
    "X, y = [], []\n",
    "with open(TRAIN_SET_PATH, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        label, text = line.split(\"\\t\")\n",
    "        # texts are already tokenized, just split on space\n",
    "        # in a real case we would use e.g. spaCy for tokenization\n",
    "        # and maybe remove stopwords etc.\n",
    "        X.append(text.split())\n",
    "        y.append(label)\n",
    "X, y = np.array(X), np.array(y)\n",
    "print (\"total examples %s\" % len(y))\n",
    "\n",
    "\n",
    "# reading glove files, this may take a while\n",
    "# we're reading line by line and only saving vectors\n",
    "# that correspond to words from our training set\n",
    "# if you wan't to play around with the vectors and have \n",
    "# enough RAM - remove the 'if' line and load everything\n",
    "\n",
    "glove_small = {}\n",
    "all_words = set(w for words in X for w in words)\n",
    "with open(GLOVE_6B_50D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_small[word] = nums\n",
    "            \n",
    "glove_big = {}\n",
    "with open(GLOVE_840B_300D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_big[word] = nums\n",
    "            \n",
    "            \n",
    "            \n",
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "model = Word2Vec(X, size=100, window=5, min_count=5, workers=2)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}\n",
    "\n",
    "\n",
    "\n",
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "etree_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_big_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_models = [\n",
    "    (\"mult_nb\", mult_nb),\n",
    "    (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "    (\"bern_nb\", bern_nb),\n",
    "    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"svc\", svc),\n",
    "    (\"svc_tfidf\", svc_tfidf),\n",
    "    (\"glove_small\", etree_glove_small), \n",
    "    (\"glove_small_tfidf\", etree_glove_small_tfidf),\n",
    "    (\"glove_big\", etree_glove_big), \n",
    "    (\"glove_big_tfidf\", etree_glove_big_tfidf),\n",
    "    (\"w2v\", etree_w2v),\n",
    "    (\"w2v_tfidf\", etree_w2v_tfidf),\n",
    "]\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, X, y, cv=5).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "print(tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))\n",
    "\n",
    "\n",
    "train_sizes = [100, 400, 1600, 3200, 6400, 8000]\n",
    "table = []\n",
    "for name, model in all_models:\n",
    "    for n in train_sizes:\n",
    "        table.append({'model': name, \n",
    "                      'accuracy': benchmark(model, X, y, n), \n",
    "                      'train_size': n})\n",
    "df = pd.DataFrame(table)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "fig = sns.pointplot(x='train_size', y='accuracy', hue='model', \n",
    "                    data=df[df.model.map(lambda x: x in [\"mult_nb\", \"svc_tfidf\", \"w2v_tfidf\", \n",
    "                                                         \"glove_small_tfidf\", \"glove_big_tfidf\", \n",
    "                                                        ])])\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "fig.set(ylabel=\"accuracy\")\n",
    "fig.set(xlabel=\"labeled training examples\")\n",
    "fig.set(title=\"R52 benchmark\")\n",
    "fig.set(ylabel=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the 20 news group dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_PATH = \"20ng-no-stop.txt\"\n",
    "\n",
    "X, y = [], []\n",
    "with open(TRAIN_SET_PATH, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        label, text = line.split(\"\\t\")\n",
    "        # texts are already tokenized, just split on space\n",
    "        # in a real case we would use e.g. spaCy for tokenization\n",
    "        # and maybe remove stopwords etc.\n",
    "        X.append(text.split())\n",
    "        y.append(label)\n",
    "X, y = np.array(X), np.array(y)\n",
    "print (\"total examples %s\" % len(y))\n",
    "\n",
    "\n",
    "# reading glove files, this may take a while\n",
    "# we're reading line by line and only saving vectors\n",
    "# that correspond to words from our training set\n",
    "# if you wan't to play around with the vectors and have \n",
    "# enough RAM - remove the 'if' line and load everything\n",
    "\n",
    "glove_small = {}\n",
    "all_words = set(w for words in X for w in words)\n",
    "with open(GLOVE_6B_50D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_small[word] = nums\n",
    "            \n",
    "glove_big = {}\n",
    "with open(GLOVE_840B_300D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_big[word] = nums\n",
    "            \n",
    "            \n",
    "            \n",
    "# train word2vec on all the texts - both training and test set\n",
    "# we're not using test labels, just texts so this is fine\n",
    "model = Word2Vec(X, size=100, window=5, min_count=5, workers=2)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}\n",
    "\n",
    "\n",
    "\n",
    "# Extra Trees classifier is almost universally great, let's stack it with our embeddings\n",
    "etree_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_big_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "etree_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_models = [\n",
    "    (\"mult_nb\", mult_nb),\n",
    "    (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "    (\"bern_nb\", bern_nb),\n",
    "    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"svc\", svc),\n",
    "    (\"svc_tfidf\", svc_tfidf),\n",
    "    (\"glove_small\", etree_glove_small), \n",
    "    (\"glove_small_tfidf\", etree_glove_small_tfidf),\n",
    "    (\"glove_big\", etree_glove_big), \n",
    "    (\"glove_big_tfidf\", etree_glove_big_tfidf),\n",
    "    (\"w2v\", etree_w2v),\n",
    "    (\"w2v_tfidf\", etree_w2v_tfidf),\n",
    "]\n",
    "\n",
    "    \n",
    "unsorted_scores = [(name, cross_val_score(model, X, y, cv=5).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "print (tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))\n",
    "\n",
    "\n",
    "\n",
    "train_sizes = [40, 160, 640, 3200, 6400, 9000]\n",
    "table = []\n",
    "for name, model in all_models:\n",
    "    for n in train_sizes:\n",
    "        table.append({'model': name, \n",
    "                      'accuracy': benchmark(model, X, y, n), \n",
    "                      'train_size': n})\n",
    "df = pd.DataFrame(table)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "fig = sns.pointplot(x='train_size', y='accuracy', hue='model', \n",
    "                    data=df[df.model.map(lambda x: x in [\"mult_nb\", \"svc_tfidf\", \"w2v_tfidf\", \n",
    "                                                         \"glove_small_tfidf\", \"glove_big_tfidf\", \n",
    "                                                        ])])\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "fig.set(ylabel=\"accuracy\")\n",
    "fig.set(xlabel=\"labeled training examples\")\n",
    "fig.set(title=\"20 news groups benchmark\")\n",
    "fig.set(ylabel=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
